** Running Instructions
To execute the Docker container run 
#+begin_src shell
docker-compose build
docker-compose up
#+end_src

The command
#+begin_src shell
docker-compose up --build
#+end_src
may result in:
#+begin_src shell
OSError: Can't load tokenizer for 'openai/clip-vit-large-patch14'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'openai/clip-vit-large-patch14' is the correct path to a directory containing all relevant files for a CLIPTokenizer tokenizer.
#+end_src

** Install CUDA toolkit

Uninstall previous version
https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#removing-cuda-toolkit-and-driver

Download and install new
https://developer.nvidia.com/cuda-downloads

** Install nvidia-container-toolkit

#+begin_src shell
  # Add nvidia toolkit repo to apt get
  distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
      && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
      && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
            sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
            sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
  # update and install 
  sudo apt-get update

  # install nvidia-cotainer-toolkit
  sudo apt-get install -y nvidia-container-toolkit

  # check version 
  nvidia-ctk --version

  # Generate a CDI specification that refers to all devices
  sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml

  # Check name of the created devices
  sudo grep "  name:" /etc/cdi/nvidia.yaml
#+end_src

Install Docker
#+begin_src shell
curl https://get.docker.com | sh \
  && sudo systemctl --now enable docker
#+end_src

Configure the Docker daemon to recognize the NVIDIA Container Runtime:
#+begin_src shell
sudo nvidia-ctk runtime configure --runtime=docker

# Restart Docker
sudo systemctl restart docker
#+end_src

If after all this process the following error appears:
#+begin_src shell
docker: Error response from daemon: could not select device driver "" with capabilities: [[gpu]].
#+end_src

Repeat this steps:
#+begin_src shell
sudo nvidia-ctk runtime configure --runtime=docker

# Restart Docker
sudo systemctl restart docker
#+end_src

*** Refs
[[Nvidia Container Toolkit Installation Guide][https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#installation-guide]]

** Sorolla Style

*** Model
[[Oil painting][https://civitai.com/models/20184/oil-painting]] 5.57GB
- Checkpoint of a Stable Diffusion 1.5 model trained with oil paintings

*** LoRA Training

[[Reference video 1][https://www.youtube.com/watch?v=N4_-fB62Hwk]]
[[Reference video 2][https://www.youtube.com/watch?v=k5imq01uvUY]]
[[Reference video 3][https://www.youtube.com/watch?v=7m522D01mh0&t=557s&ab_channel=kasukanra]]

Install the [[Training webui for the LoRA training scripts][https://github.com/bmaltais/kohya_ss#launching-the-gui-on-linux-and-macos]]. ([[original training scripts repo][https://github.com/kohya-ss/sd-scripts]])
#+begin_src shell
git clone https://github.com/bmaltais/kohya_ss.git
cd kohya_ss
# May need to chmod +x ./setup.sh if you're on a machine with stricter security.
# There are additional options if needed for a runpod environment.
# Call 'setup.sh -h' or 'setup.sh --help' for more information.
./setup.sh
#+end_src


**** Dataset Preparation

1 - Crop images to desired resolution.
  - [[BIRME.net][https://www.birme.net/]] allows to resize and crop multiple images and visually select the portion that's croped out

2 - Generate prompts for the images using the Bmaltais GUI captioning tool.
    - Go to the Utilities/Captioning/BLIP Captioning
    - Add "SOROLLA" in the =Prefix to add to BLIP caption= box. 
    - Set =Min length= to 22 so the generated prompt is long and descriptive enough
    - Leave other parametters at deffault
    - Click =Caption Images=
    - Captions will be generated as =.txt= files in the images folder.

3 - Review generated prompts
    - Remove all text like "a painting of ". Making it represent only the object so it learns the sytle on anything

4 - Generate training folder structure using =Tools= tab in the WebUI.
    - Desired number of reapeats per each image in =Repeats= (used 50)
    - "SorollaLoRA" in =Instance prompt=
    - "artwork style" in =Class prompt=
    - Alternatively ensure that the folde struncture is:
        |_img 
            |_ <NUMBER OF REAPEATS>_<desired name> (folder with all the captioned images)
        |_ model
        |_ log

    - This is not strictly necessary, the only necessaty folder is the img/<NUMBER OF REAPEATS>_<any name>, this tells the WebUI how many repeats to do, the "model" folder and "log" folder can then be what you prefer on the =Folders tab= of the Training section.


**** Training

***** Source Model tab
Select the training model. Models used in experiments
  - 1.5 model Used model: stable-diffusion-webui/models/Stable-diffusion/v1-5-pruned-emaonly.safetensors (the downloaded with the AUTOMATIC1111 webui)

***** Folders tab
Set:
- =Image folder= to the "img" folder created before
- =Logging folder= to the "log" folder created before. (Can be anything actualy but will keep things clean)
- =Output folder= to the "model" folder created before. (Can be anything actualy but will keep things clean)
- =Model output name= to "sorolla_lora_01" (Whatevet you like)

***** Training Parametters
- =Batch Size=: 1 (depends on GPU)
- =Epochs=: 2 (affects training time)
- =Caption extension=: ".txt"

- =Mixed Precission= & =Save precision=: Try bf16 but low VRAM may need fp16
- =Number of CPU threads per core=: 1 or 2
- =Seed=: 420 (use a set seed to easier compare improvements)

- =Learning rate=: 0.0001
- =LR Scheduler=: Constant
- =LR warmup (% of steps)=: 0%
- =Optimizer=: AdamW8bit
- =Cache Latents=: Depends on VRAM memory, selcted will require more VRAM and train faster.

- =Text Encoder learning rate=: 5e-5
- =Unet learning rate=: 0.0001
- =Network Rank (Dimension)=: 200
- =Network Alpha=: 200

- =Max Resolution= 512,512


Advanced Configuration:

Left everythin and default and made sure the following parametters where set. Must try with and without them:
- =Clip Skip=: 1 for stable diffusion v1.x models and 2 for v2.x models
- =Shuffle caption=
- =Color augmentation=: may help preserve color style
- =Flip augmentation=
- =Max num workers for DataLoader=: 1
- =Memory efficient attention=: Saves memory

*You can save the training parametters in a .json file to load them the next time*. Go To the =Configuration file= menu of the =Training Parametters= tab

*** Usage
**** WebGUI
[[Github Repostitory][https://github.com/AUTOMATIC1111/stable-diffusion-webui]]
[[Instructions for installation on Apple Silicon][https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon]]

- To use the Oil Model place it in the =models/Stable-diffusion= folder.

*Parametters used in the experiment*
- [[Used image in tests][https://del.h-cdn.co/assets/17/07/3200x3200/square-1487365300-delish-ryan-gosling-getty-pascal.jpg]]
- Text prompt: "An oil painting portrait by joaquin sorolla."
- CFG Scale: ~18
  - Controls balance between text prompt and image, 0 means no text prompt effect.
- Denoising Strenght: ~0.12
  - Determines how little respect the algorithm should have for image's content. At 0, nothing will change, and at 1 you'll get an unrelated image [[Ref][https://www.mayerdan.com/software/2023/02/27/making-book-covers-with-img2img#:~:text=Stable%20Diffusion%20Denoising%20Strength%20is,the%20Sampling%20Steps%20slider%20specifies]]
  - Values arround 0.25 give results with a more solid background, less importance of the original backgound
- Other parametters are left on deffault (sampling steps 20; batch count 1; batch size 1; width & height 512)


Alternatively the code can be executed directly from the command line using the Official Stable Diffusion Github repo or the optimized version

**** Official Repo
[[Github Repostitory][https://github.com/CompVis/stable-diffusion.git]]

**** Optimization
[[Github Repostitory][https://github.com/basujindal/stable-diffusion]]

This repo is a modified version of the Stable Diffusion repo, optimized to use less VRAM than the original by sacrificing inference speed.

*All the modified files are in the =optimizedSD= folder, so if you have already cloned the original repository you can just download and copy this folder into the original instead of cloning the entire repo*. You can also clone this repo and follow the same installation steps as the original (mainly creating the conda environment and placing the weights at the specified location).

Perform img2img with optimizedSD scripts:

#+begin_src shell
  python optimizedSD/optimized_img2img.py --prompt "<prompt>" --init-img "<init image path>" --outdir "<otuput directory>" --ckpt "<model checkpoint>" --strength 0.12 --scale 18 --n_samples 1 --n_iter 1 --H 512 --W 512
#+end_src

*To use the =oilPainting_oilPaintingV10.safetensors= model with the original repo or the optimized version it must be converted to `.ckpt` file.* 

**** Convert safetensor to ckpt
[[Github Repostitory][https://github.com/diStyApps/Safe-and-Stable-Ckpt2Safetensors-Conversion-Tool-GUI]]

Install the missing requirements to the stable diffusion environment
#+begin_src shell
pip install safetensors
pip install PySimpleGUI
#+end_src

Run GUI
#+begin_src shell
python run_app_gui.py
#+end_src

Browse to the folder with the =oilPainting_oilPaintingV10.safetensors=, select it and click =Convert File=, the result file will be created in the same folder as the original one.

With all this steps, the command executed to replicate the same results as in the webgui in may look like:
#+begin_src shell
python optimizedSD/optimized_img2img.py --prompt "An oil painting portrait by joaquin sorolla." --init-img "input_images/square-1487365300-delish-ryan-gosling-getty-pascal.jpg" --outdir "outputs/stable-diffusion-tests/ryan_gosling_oil_model_test_replication" --ckpt "models/Stable-diffusion/oilPainting_oilPaintingV10.ckpt" --strength 0.12 --scale 18 --n_samples 1 --n_iter 1 --H 512 --W 512
#+end_src
